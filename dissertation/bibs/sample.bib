
@article{full_text_search,
title = {The Weaknesses of Full-Text Searching},
journal = {The Journal of Academic Librarianship},
volume = {34},
number = {5},
pages = {438-444},
year = {2008},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2008.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0099133308001067},
author = {Jeffrey Beall},
abstract = {This paper provides a theoretical critique of the deficiencies of full-text searching in academic library databases. Because full-text searching relies on matching words in a search query with words in online resources, it is an inefficient method of finding information in a database. This matching fails to retrieve synonyms, and it also retrieves unwanted homonyms. Numerous other problems also make full-text searching an ineffective information retrieval tool. Academic libraries purchase and subscribe to numerous proprietary databases, many of which rely on full-text searching for access and discovery. An understanding of the weaknesses of full-text searching is needed to evaluate the search and discovery capabilities of academic library databases.}
}

@article{categories_chatbots,
title = {Chatbots: History, technology, and applications},
journal = {Machine Learning with Applications},
volume = {2},
pages = {100006},
year = {2020},
issn = {2666-8270},
doi = {https://doi.org/10.1016/j.mlwa.2020.100006},
url = {https://www.sciencedirect.com/science/article/pii/S2666827020300062},
author = {Eleni Adamopoulou and Lefteris Moussiades},
keywords = {Chatbot, Pattern matching, Machine learning, Natural dialog interfaces, Natural language processing, Human–computer interaction},
abstract = {This literature review presents the History, Technology, and Applications of Natural Dialog Systems or simply chatbots. It aims to organize critical information that is a necessary background for further research activity in the field of chatbots. More specifically, while giving the historical evolution, from the generative idea to the present day, we point out possible weaknesses of each stage. After we present a complete categorization system, we analyze the two essential implementation technologies, namely, the pattern matching approach and machine learning. Moreover, we compose a general architectural design that gathers critical details, and we highlight crucial issues to take into account before system design. Furthermore, we present chatbots applications and industrial use cases while we point out the risks of using chatbots and suggest ways to mitigate them. Finally, we conclude by stating our view regarding the direction of technology so that chatbots will become really smart.}
}

@article{shrungare,
author = {Shrungare, Jeenisha},
title = {AI in Education},
year = {2023},
issue_date = {Spring 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1528-4972},
url = {https://doi.org/10.1145/3589657},
doi = {10.1145/3589657},
journal = {XRDS},
month = {apr},
pages = {63–65},
numpages = {3}
}

@article{harry,
  title={Role of AI in Education},
  author={Harry, Alexandara},
  journal={Interdiciplinary Journal and Hummanity (INJURITY)},
  volume={2},
  number={3},
  pages={260--268},
  year={2023}
}

@ARTICLE{Chen,
  author={Chen, Lijia and Chen, Pingping and Lin, Zhijian},
  journal={IEEE Access}, 
  title={Artificial Intelligence in Education: A Review}, 
  year={2020},
  volume={8},
  number={},
  pages={75264-75278},
  keywords={Education;Technological innovation;Learning (artificial intelligence);Microcomputers;Robots;Education;artificial intelligence;leaner},
  doi={10.1109/ACCESS.2020.2988510}}

@INPROCEEDINGS{jaakkola,
  author={Jaakkola, H. and Henno, J. and Mäkelä, J.},
  booktitle={2023 46th MIPRO ICT and Electronics Convention (MIPRO)}, 
  title={Computers in Education}, 
  year={2023},
  volume={},
  number={},
  pages={833-839},
  keywords={Computers;Training;Soft sensors;Education;Random access memory;Microcomputers;Information and communication technology;computers;education;computing trends;teacher training},
  doi={10.23919/MIPRO57284.2023.10159980}}

@inproceedings{humsum,
    title = "{H}um{S}um: A Personalized Lecture Summarization Tool for Humanities Students Using {LLM}s",
    author = "Kolagar, Zahra  and
      Zarcone, Alessandra",
    editor = "Deshpande, Ameet  and
      Hwang, EunJeong  and
      Murahari, Vishvak  and
      Park, Joon Sung  and
      Yang, Diyi  and
      Sabharwal, Ashish  and
      Narasimhan, Karthik  and
      Kalyan, Ashwin",
    booktitle = "Proceedings of the 1st Workshop on Personalization of Generative AI Systems (PERSONALIZE 2024)",
    month = mar,
    year = "2024",
    address = "St. Julians, Malta",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.personalize-1.4",
    pages = "36--70",
    abstract = "Generative AI systems aim to create customizable content for their users, with a subsequent surge in demand for adaptable tools that can create personalized experiences. This paper presents HumSum, a web-based tool tailored for humanities students to effectively summarize their lecture transcripts and to personalize the summaries to their specific needs. We first conducted a survey driven by different potential scenarios to collect user preferences to guide the implementation of this tool. Utilizing Streamlit, we crafted the user interface, while Langchain{'}s Map Reduce function facilitated the summarization process for extensive lectures using OpenAI{'}s GPT-4 model. HumSum is an intuitive tool serving various summarization needs, infusing personalization into the tool{'}s functionality without necessitating the collection of personal user data.",
}

@ARTICLE{murtaza,
  author={Murtaza, Mir and Ahmed, Yamna and Shamsi, Jawwad Ahmed and Sherwani, Fahad and Usman, Mariam},
  journal={IEEE Access}, 
  title={AI-Based Personalized E-Learning Systems: Issues, Challenges, and Solutions}, 
  year={2022},
  volume={10},
  number={},
  pages={81323-81342},
  keywords={Electronic learning;Education;Videos;Adaptation models;Object recognition;Artificial intelligence;Learning (artificial intelligence);Recommender systems;Data mining;Adaptability;artificial intelligence;educational data mining;knowledge tracing;personalized e-learning;recommender systems},
  doi={10.1109/ACCESS.2022.3193938}}

@article{bundit,
author = {Anuyahong, Asst.Prof.Dr.Bundit and Rattanapong, Chalong and Patcha, Inteera},
year = {2023},
month = {05},
pages = {88-93},
title = {Analyzing the Impact of Artificial Intelligence in Personalized Learning and Adaptive Assessment in Higher Education},
volume = {X},
journal = {International Journal of Research and Scientific Innovation},
doi = {10.51244/IJRSI.2023.10412}
}

@article{txt_class,
doi = {10.1088/1742-6596/1881/3/032020},
url = {https://dx.doi.org/10.1088/1742-6596/1881/3/032020},
year = {2021},
month = {apr},
publisher = {IOP Publishing},
volume = {1881},
number = {3},
pages = {032020},
author = {Yibao Huang},
title = {Application of Natural Language Processing Technology in Educational Resources Retrieval},
journal = {Journal of Physics: Conference Series},
abstract = {According to the data structure characteristics of educational resources, natural language processing technology is used to process the original educational resources data from the aspects of word segmentation processing, named entity recognition, part of speech tagging, synonym analysis, word vector analysis, etc. For complex image data, OCR preprocessing and coding format conversion are used to reduce the complexity of the original data; according to the difficulty requirements of the user to retrieve data, key words or ontology are selected to further expand and enrich the semantics and improve the accuracy of the retrieval results.}
}

@article{tokn_class, volume={9},  url={https://acspublisher.com/journals/index.php/ijircst/article/view/11000},  DOI={10.55524/}, abstractNote={NLP (Natural Language Processing) is an&amp;amp;nbsp; intriguing technique for improving educational settings.&amp;amp;nbsp; In educational modelling, actualizing NLP entails&amp;amp;nbsp; beginning path toward learning via regular acquisition. It&amp;amp;nbsp; is dependent on operative methods in providing answers&amp;amp;nbsp; to numerous training problems. Conventional Processing&amp;amp;nbsp; allows for organization in a broad variety of areas linked&amp;amp;nbsp; to language learning&amp;#039;s social &amp;amp;amp; social context. It&amp;#039;s a&amp;amp;nbsp; proven approach for instructors, students, authors, &amp;amp;amp; teachers to assist with producing, analysis, &amp;amp;amp; evaluation&amp;amp;nbsp; methods. NLPis widely used in a wide range of&amp;amp;nbsp; educational institutions, including research, phonetics,&amp;amp;nbsp; eLearning, &amp;amp;amp; assessment modelling, &amp;amp;amp; it leads to good&amp;amp;nbsp; outcomes in or educational organizations, such as&amp;amp;nbsp; schools, advanced education institutes, &amp;amp;amp; colleges.&amp;amp;nbsp; Purpose of this article is to discuss process of acquiring a&amp;amp;nbsp; common language &amp;amp;amp; its implementation in educational&amp;amp;nbsp; institutions. Study approach also discusses how NLP&amp;amp;nbsp; may be utilized in conjunction with logical PC projects to&amp;amp;nbsp; enhance training process. Subjective approach is pursued&amp;amp;nbsp; in research methodology. Information is collected from&amp;amp;nbsp; auxiliary assets in order to identify problems that teachers&amp;amp;nbsp; &amp;amp;amp; students have in comprehending environment due to&amp;amp;nbsp; language barriers. Findings show that traditional&amp;amp;nbsp; apparatuses, such as language, sentence structure, &amp;amp;amp; printed designs, are viable in educational settings for&amp;amp;nbsp; learning &amp;amp;amp; assessment.&amp;amp;nbsp;}, number={6}, journal={International Journal of Innovative Research in Computer Science &amp; Technology}, year={2021}, month={Nov.}, pages={184–187} }

@misc{que-ans,
      title={QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering}, 
      author={Michihiro Yasunaga and Hongyu Ren and Antoine Bosselut and Percy Liang and Jure Leskovec},
      year={2022},
      eprint={2104.06378},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zero-shot,
      title={Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections}, 
      author={Ruiqi Zhong and Kristy Lee and Zheng Zhang and Dan Klein},
      year={2021},
      eprint={2104.04670},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{trans,
    author = {He, Zhiwei and Liang, Tian and Jiao, Wenxiang and Zhang, Zhuosheng and Yang, Yujiu and Wang, Rui and Tu, Zhaopeng and Shi, Shuming and Wang, Xing},
    title = "{Exploring Human-Like Translation Strategy with Large Language
                    Models}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {12},
    pages = {229-246},
    year = {2024},
    month = {03},
    abstract = "{Large language models (LLMs) have demonstrated impressive capabilities in general
                    scenarios, exhibiting a level of aptitude that approaches, in some aspects even
                    surpasses, human-level intelligence. Among their numerous skills, the
                    translation abilities of LLMs have received considerable attention. Compared to
                    typical machine translation that focuses solely on source-to-target mapping,
                    LLM-based translation can potentially mimic the human translation process, which
                    might take preparatory steps to ensure high-quality translation. This work
                    explores this possibility by proposing the MAPS framework, which
                    stands for Multi-Aspect Prompting and Selection. Specifically, we enable LLMs first to analyze the
                    given source sentence and induce three aspects of translation-related knowledge
                    (keywords, topics, and relevant demonstrations) to guide the final translation
                    process. Moreover, we employ a selection mechanism based on quality estimation
                    to filter out noisy and unhelpful knowledge. Both automatic (3 LLMs × 11
                    directions × 2 automatic metrics) and human evaluation (preference study
                    and MQM) demonstrate the effectiveness of MAPS. Further analysis shows that by
                    mimicking the human translation process, MAPS reduces various translation errors
                    such as hallucination, ambiguity, mistranslation, awkward style, untranslated
                    text, and omission. Source code is available at https://github.com/zwhe99/MAPS-mt.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00642},
    url = {https://doi.org/10.1162/tacl\_a\_00642},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00642/2346100/tacl\_a\_00642.pdf},
}

@InProceedings{summ,
author="Syed, Ayesha Ayub
and Gaol, Ford Lumban
and Boediman, Alfred
and Matsuo, Tokuro
and Budiharto, Widodo",
editor="Nguyen, Ngoc Thanh
and Tran, Tien Khoa
and Tukayev, Ualsher
and Hong, Tzung-Pei
and Trawi{\'{n}}ski, Bogdan
and Szczerbicki, Edward",
title="A Survey of Abstractive Text Summarization Utilising Pretrained Language Models",
booktitle="Intelligent Information and Database Systems",
year="2022",
publisher="Springer International Publishing",
address="Cham",
pages="532--544",
abstract="We live in a digital era - an era of technology, artificial intelligence, big data, and information. The data and information on which we depend to fulfil several daily tasks and decision-making can become overwhelming to deal with and requires effective processing. This can be achieved by designing improved and robust automatic text summarization systems. These systems reduce the size of text document while retaining the salient information. The resurgence of deep learning and its progress from the Recurrent Neural Networks to deep transformer based Pretrained Language Models (PLM) with huge parameters and ample world and common-sense knowledge have opened the doors for huge success and improvement of the Natural Language Processing tasks including Abstractive Text Summarization (ATS). This work surveys the scientific literature to explore and analyze recent research on pre-trained language models and abstractive text summarization utilizing these models. The pretrained language models on abstractive summarization tasks have been analyzed quantitatively based on ROUGE scores on four standard datasets while the analysis of state-of-the-art ATS models has been conducted qualitatively to identify some issues and challenges encountered on finetuning large PLMs on downstream datasets for abstractive summarization. The survey further highlights some techniques that can help boost the performance of these systems. The findings in terms of performance improvement reveal that the models with better performance use either one or a combination of these strategies: (1) Domain Adaptation, (2) Model Augmentation, (3) Stable finetuning, and (4) Data Augmentation.",
isbn="978-3-031-21743-2"
}

@misc{txt_gen,
      title={Exploring Controllable Text Generation Techniques}, 
      author={Shrimai Prabhumoye and Alan W Black and Ruslan Salakhutdinov},
      year={2020},
      eprint={2005.01822},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{amazon,
  title = {{What is Retrieval-Augmented Generation?} -AMAZON-AWS},
  howpublished = {\url{https://aws.amazon.com/what-is/retrieval-augmented-generation/}},
  note = {}
}

@misc{IBM_rnd,
  title = {{What is retrieval-augmented generation?} -IBM},
  howpublished = {\url{https://research.ibm.com/blog/retrieval-augmented-generation-RAG}},
  note = {}
}

@misc{nvidia,
  title = {{RAG 101: Demystifying Retrieval-Augmented Generation Pipelines} -NVIDIA},
  howpublished = {\url{https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/}},
  note = {}
}

@misc{beebom,
  title = {{12 Best Large Language Models (LLMs) in 2024} -Beebom},
  howpublished = {\url{https://beebom.com/best-large-language-models-llms/}},
  note = {}
}

@misc{fine_tune,
  title = {{Inside Zephyr-7B: HuggingFace’s Hyper-Optimized LLM that Continues to Outperform Larger Models}},
  howpublished = {\url{https://towardsai.net/p/machine-learning/inside-zephyr-7b-huggingfaces-hyper-optimized-llm-that-continues-to-outperform-larger-models}},
  note = {}
}

@misc{foundation,
  title = {{Exploring the World of Generative AI, Foundation Models, and Large Language Models: Concepts, Tools, and Trends}},
  howpublished = {\url{https://towardsai.net/p/machine-learning/exploring-the-world-of-generative-ai-foundation-models-and-large-language-models-concepts-tools-and-trends}},
  note = {}
}

@misc{django,
  title = {{Django makes it easier to build better web apps more quickly and with less code.}},
  howpublished = {\url{https://www.djangoproject.com/}},
  note = {}
}