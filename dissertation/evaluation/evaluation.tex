\chapter{Testing and Evaluation}

This chapter details the evaluation methodology and testing processes used to assess the efficacy of the newly developed multi-modal system, which integrates a fine-tuned large language model (LLM) with keyword and context search functionalities. The goal is to compare the performance of the multi-modal fine-tuned LLM against the baseline LLM and ChatGPT-4 across various metrics to ascertain improvements in search relevance, user satisfaction, and system response times. Unit testing is also done to test each component individually.


\section{LLM Evaluation:}
The evaluation utilized a corpus of documents and real-world queries collected from the internet, categorized by relevance and from the subject "Probability and statistics". Data preprocessing involved normalization of text and removal of identifiable metadata to maintain privacy and consistency. 

General queries were sent to the LLM with contexts, and the responses generated were descriptive, indicating a successful test result. Two models were evaluated, A base LLM model, presumably pre-trained on a diverse corpus of texts and a fine-tuned LLM, specifically optimized for improved performance on domain-specific queries.

\subsection{Base Model}

The base model provided a concise response: "mathematically" This answer, isn't what was expected, lacks depth fails to fully utilize the context provided and eventually fails the test.

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/ft10.png}
      \caption{Responses generated Base model}
      \label{ft10}
\end{figure}

\subsection{Fine-Tuned Model}

The fine-tuned model's response was correct and descriptive: 

" Axiom 1: The probability of an event is a real number greater than or equal to 0. Axiom 2: The probability that at least one of all the possible outcomes of a process (such as rolling a die) will occur is 1. Axiom 3: If two events A and B are mutually exclusive, then the probability of either A or B occurring is the probability of A occurring plus the probability of B". This answer not only addressed the query directly but also enriched the response with relevant details from the context, demonstrating an improved understanding and application of the concept.

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/ft11.png}
      \caption{Descriptive Responses generated by our fine-tuned model}
      \label{ft11}
\end{figure}


\section{Keyword Search and Context Retrieval}

This section of the report discusses the evaluation of the keyword search and Context Retrieval functionality within a large language model (LLM) designed for educational agents. The objective was to assess the model's ability to identify and extract key terms from natural language queries and return relevant contexts.

\subsubsection{Keyword Search}
A Python script was executed which utilized the Natural Language Toolkit (NLTK) for text processing and the Scikit-learn library for extracting keywords using the Term Frequency-Inverse Document Frequency (TF-IDF) method. The query given to the system was: "What is central tendency? What are its measures?"

The keyword search process successfully identified the following top four keywords from the input query:
['central', 'measures', 'tendency']

These keywords accurately reflect the essential components of the query related to the concept of central tendency in statistics.

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/tst1.png}
      \caption{Keyword Search Results}
      \label{tst1}
\end{figure}

\subsection{Context Retrieval}
In continuation of our keyword search evaluation, we progressed to examine the context retrieval capabilities of our Large Language Model (LLM). Using the keywords identified in the previous stage, we tested the model's ability to extract relevant sections from a text, aiming to provide informative content based on the keywords.

We applied a Whoosh-based indexing and search system to a segmented text corpus. The corpus was structured into chapters and sections for granular retrieval. The input for the context retrieval was the set of keywords previously extracted: "central" "measures" and "tendency".

\subsubsection{The context retrieval system effectively located several relevant sections within the corpus:}

Chapter 3, Section 5: Detailed measures of central tendency and their application in statistical analysis.

Chapter 3, Section 42: Discussed the normal distribution, its properties, and its relation to measures of central tendency.

Chapter 5, Section 1: Outlined the types of descriptive statistics, emphasizing central tendency and its measures.

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/tst2.png}
      \caption{Context Retrieval Results}
      \label{tst2}
\end{figure}

\section{Comparative Analysis of Responses from Base Model, Fine-Tuned Model, and ChatGPT-4}

The models were provided with a document containing specific information related to statistics and public health. The evaluation involved querying each model with identical questions to assess their ability to extract and present information correctly and contextually.

Each model was queried with the same set of questions, and the responses were compared to evaluate their precision, adherence to the provided document, and the depth of information provided.

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/fine-tuned.png}
      \caption{Simple Query Response (Base and Fine-tuned Model)}
      \label{base-fine}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/chatgpt-4.png}
      \caption{Simple Query Response (ChatGPT-4)}
      \label{gpt4}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/tst3.png}
      \caption{Simple Query-Response Analysis}
      \label{tst3}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/fine-tuned(2).png}
      \caption{Multiple Keyword Query-Response (Base and Fine-tuned Model)}
      \label{ftr}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/chatgpt-4(2).png}
      \caption{Multiple Keyword Query-Response (ChatGPT-4)}
      \label{ftr2}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/tst4.png}
      \caption{Multiple Keyword Query-Response Analysis}
      \label{tst4}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/fine-tuned(3).png}
      \caption{Document Based Query-Response (Base and Fine-tuned Model)}
      \label{ftr3}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/gpt-4(3).png}
      \caption{Document Based Query-Response (ChatGPT-4)}
      \label{ftr4}
\end{figure}

\begin{figure}[H]
      \centering
    \includegraphics[width=0.9\textwidth]{evaluation/tst5.png}
      \caption{Document Based Query-Response Analysis}
      \label{tst5}
\end{figure}

The fine-tuned model and ChatGPT-4 generally provided more detailed and contextually relevant responses compared to the base model. This suggests that the fine-tuning process and advanced capabilities of ChatGPT-4 allow for better utilization of provided documents, resulting in more informative and precise answers.
